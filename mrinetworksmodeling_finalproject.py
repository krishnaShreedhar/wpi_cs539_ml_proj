# -*- coding: utf-8 -*-
"""MRINetworksModeling_FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sjVVtYKBRdo-25U-Miwu3NWTltUI3BuE

## Setting up Environment
"""

# Importing dataset
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import SGD, RMSprop, Adam

import nibabel as nib
from scipy import ndimage
import random
import math

import numpy as np
import nibabel as nib
import itertools
import os

# from google.colab import drive
# drive.mount('/content/drive')

if tf.test.gpu_device_name():
    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))
else:
    print("Please install GPU version of TF")
print(tf.executing_eagerly())
print()

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

# !unzip '/content/drive/MyDrive/Copia de T1wCE.zip'

"""# **Validate loading of Training and Testing Data**"""

data_folder = os.getcwd() + '/data/project_folder_T1wCE/cross_val_folds/fold_1/'
train_img_len = len(os.listdir(data_folder + "train/" + "0/")) + len(os.listdir(data_folder + "train/" + "1/"))
val_img_len = len(os.listdir(data_folder + "val/" + "0/")) + len(os.listdir(data_folder + "val/" + "1/"))

print()
print("Train image length: ", train_img_len)
print("Validation image length: ", val_img_len)

"""# **Setting preprocessing training parameters**"""

# Training parameters (for current simple model provided by the Jupyter notebook)
batch_size = 2
steps_per_epoch = int(math.ceil(train_img_len/batch_size))
validation_steps = int(math.ceil(val_img_len/batch_size))
w_width = 112
h_height = 112
d_depth = 96

"""# **Utility preprocessing image functions**"""

import os
import warnings
import numpy as np
import nibabel as nib

from multiprocessing import Pool, cpu_count
from scipy.ndimage.interpolation import zoom


def load_nii(path):
        '''LOAD_NII
            Load image to numpy ndarray from NIfTi file.
            Input:
            ------
            - path: string , path of input image.
            Ouput:
            ------
            - A numpy array of input imgae.
        '''

        return np.rot90(nib.load(path).get_data(), 3)


def resize(volume, target_shape=[112, 112, 96]):
        '''RESIZE
            Resize input image to target shape.
            -1- Resize to [112, 112, 96].
            -2- Crop image to [112, 96, 96].
        '''

        # Shape of input image
        old_shape = list(volume.shape)

        # Resize image
        factor = [n / float(o) for n, o in zip(target_shape, old_shape)]
        resized = zoom(volume, zoom=factor, order=1, prefilter=False)

        # Crop image
        resized = resized[:, 8:104, :]

        return resized

def process_scan3(in_path, is_mask=True, non_mask_coeff=0.333):
        '''_PREPROCESS
            For each input image, four steps are done:
            -1- If is_mask, enhance tumor region.
            -2- Remove background.
            -3- Resize image.
            -4- Save image.
            Inputs:
            -------
            - in_path: string, path of input image.
            - to_path: string, path of output image.
            - mask_path: string, path of the mask of input image.
            - is_mask: boolearn, if True, enhance tumor region.
                       Default is True.
            - non_mask_coeff: float from 0 to 1, the coefficient of
                              voxels in non-tumor region. Default is 0.333.
        '''

        try:
            print("Preprocessing on: " + in_path)
            # Load image
            volume = load_nii(in_path)
            # Resize image
            volume = resize(volume, [128, 128, 128])

        except RuntimeError:
            print("\tFailed to rescal:" + in_path)
            return

        return volume

def read_nifti_file(filepath):
    """Read and load volume"""
    # Read file
    scan = nib.load(filepath)
    # Get raw data
    scan = scan.get_fdata()
    return scan

def normalize(volume):
    """Normalize the volume"""
    min = np.min(volume)
    max = np.max(volume)
    volume = (volume) / (max)
    volume = volume.astype("float32")
    return volume

def normalize_max_min(volume):
    """Normalize the volume by max and min"""
    min = np.min(volume)
    max = np.max(volume)
    volume = (volume) - min / (max - min)
    volume = volume.astype("float32")
    return volume

def resize_volume(img):
    """Resize across z-axis"""
    # Set the desired depth
    desired_depth = 128
    desired_width = 128
    desired_height = 128
    # Get current depth
    current_depth = img.shape[-1]
    current_width = img.shape[0]
    current_height = img.shape[1]
    # Compute depth factor
    depth = current_depth / desired_depth
    width = current_width / desired_width
    height = current_height / desired_height
    depth_factor = 1 / depth
    width_factor = 1 / width
    height_factor = 1 / height
    # Rotate
    img = ndimage.rotate(img, 90, reshape=False)
    # Resize across z-axis
    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)
    return img

def resize_data(data):
    initial_size_x = data.shape[0]
    initial_size_y = data.shape[1]
    initial_size_z = data.shape[-1]

    new_size_x = 128
    new_size_y = 128
    new_size_z = 128

    delta_x = initial_size_x / new_size_x
    delta_y = initial_size_y / new_size_y
    delta_z = initial_size_z / new_size_z

    new_data = np.zeros((new_size_x, new_size_y, new_size_z))

    for x, y, z in itertools.product(range(new_size_x),
                                     range(new_size_y),
                                     range(new_size_z)):
        new_data[x][y][z] = data[int(x * delta_x)][int(y * delta_y)][int(z * delta_z)]
    
    return new_data

def process_scan(path):
    """Read and resize volume"""
    # Read scan
    volume = read_nifti_file(path)
    # Normalize
    volume = normalize_max_min(volume)
    # Resize width, height and depth
    volume = resize_volume(volume)
    return volume

def process_scan2(path):
    """Read and resize volume"""
    # Read scan
    volume = read_nifti_file(path)

    # Extract dimensions
    x, y, z = volume.shape

    return volume.shape

"""# **Images Preprocessing**"""

# Processing .nii images for train/val/test sets
ratings = [0, 1]
project_folder = os.getcwd() + '/data/project_folder_T1wCE/'

def preprocess_scan_images(scan_path):
  paths = []
  y = []
  for rating in ratings:
      p = project_folder + scan_path + str(rating) + "/"
      files = os.listdir(p)
      for f in files:
        y.append(float(rating))
        paths.append(p+f)
  print("Paths: ", paths)
  print(len(paths), len(y))
  print()

  return np.array([process_scan(path) for path in paths]), y

# Preprocess test images
patient_scans_test, y_test = preprocess_scan_images('cross_val_folds/test/')

# Preprocess train images
patient_scans_train, y_train = preprocess_scan_images('cross_val_folds/fold_1/train/')

# Preprocess val images
patient_scans_val, y_val = preprocess_scan_images('cross_val_folds/fold_1/val/')

x_train = patient_scans_train
y_train = np.array(y_train)
x_val = patient_scans_val
y_val = np.array(y_val)

print("------------------------------------------------")
#print("Paths train:", paths_train)
print("Length x_train: ", len(x_train))
print("y_train: ", y_train)
#print("Paths val:", paths_val)
print("Length x_val: ", len(x_val))
print("y_val: ", y_val)
print("------------------------------------------------")

def train_preprocessing(volume, label):
    """Process training data by rotating and adding a channel."""
    # Rotate volume
    # volume = rotate(volume)
    volume = tf.expand_dims(volume, axis=3)
    return volume, label

def validation_preprocessing(volume, label):
    """Process validation data by only adding a channel."""
    volume = tf.expand_dims(volume, axis=3)
    return volume, label

# Define data loaders.
train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))
validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))

# Augment them on the fly during training.
train_dataset = (
    train_loader.shuffle(len(x_train))
    .map(train_preprocessing)
    .batch(batch_size)
    .prefetch(2)
)
# Only rescale.
validation_dataset = (
    validation_loader.shuffle(len(x_val))
    .map(validation_preprocessing)
    .batch(batch_size)
    .prefetch(2)
)

def get_model(width=128, height=128, depth=32):
    """Build a 3D convolutional neural network model."""

    inputs = keras.Input((width, height, depth, 1))

    x = layers.Conv3D(filters=64, kernel_size=3, activation="relu")(inputs) # 32 filters
    x = layers.MaxPool3D(pool_size=2)(x)
    x = layers.BatchNormalization()(x)

    x = layers.Conv3D(filters=128, kernel_size=3, activation="relu")(x) # 32 filters
    x = layers.MaxPool3D(pool_size=2)(x)
    x = layers.BatchNormalization()(x)

    x = layers.Conv3D(filters=256, kernel_size=3, activation="relu")(x) # 64 filters
    x = layers.MaxPool3D(pool_size=2)(x)
    x = layers.BatchNormalization()(x)

    x = layers.GlobalAveragePooling3D()(x)
    x = layers.Dense(units=512, activation="relu")(x) #units=64
    x = layers.Dropout(0.3)(x)

    outputs = layers.Dense(units=1, activation="sigmoid")(x)

    # Define the model.
    model = keras.Model(inputs, outputs, name="3dcnn")
    return model

def get_vgg_model(width=128, height=128, depth=32):
    inputs = keras.Input((width, height, depth, 1))
    # Block 1
    x = layers.Conv3D(64, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block1_conv1')(inputs)
    x = layers.Conv3D(64, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block1_conv2')(x)
    x = layers.MaxPooling3D((2, 2, 2), strides=(2, 2, 2), name='block1_pool')(x)

    # Block 2
    x = layers.Conv3D(128, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block2_conv1')(x)
    x = layers.Conv3D(128, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block2_conv2')(x)
    x = layers.MaxPooling3D((2, 2, 2), strides=(2, 2, 2), name='block2_pool')(x)

    # Block 3
    x = layers.Conv3D(256, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block3_conv1')(x)
    x = layers.Conv3D(256, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block3_conv2')(x)
    x = layers.Conv3D(256, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block3_conv3')(x)
    x = layers.MaxPooling3D((2, 2, 2), strides=(2, 2, 2), name='block3_pool')(x)

    # Block 4
    x = layers.Conv3D(512, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block4_conv1')(x)
    x = layers.Conv3D(512, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block4_conv2')(x)
    x = layers.Conv3D(512, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block4_conv3')(x)
    x = layers.MaxPooling3D((2, 2, 2), strides=(2, 2, 2), name='block4_pool')(x)

    # Block 5
    x = layers.Conv3D(512, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block5_conv1')(x)
    x = layers.Conv3D(512, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block5_conv2')(x)
    x = layers.Conv3D(512, (3, 3, 3),
                      activation='relu',
                      padding='same',
                      name='block5_conv3')(x)
    x = layers.MaxPooling3D((2, 2, 2), strides=(2, 2, 2), name='block5_pool')(x)

    # Classification block
    x = layers.Flatten(name='flatten')(x)
    x = layers.Dense(4096, activation='relu', name='fc1')(x)
    x = layers.Dense(4096, activation='relu', name='fc2')(x)
    outputs = layers.Dense(units=1, activation="sigmoid")(x)

    # Create model.
    return keras.Model(inputs, outputs, name='vgg16')

# Build model.
model = get_vgg_model(width=w_width, height=h_height, depth=d_depth)
model.summary()

# Compile model.
epochs = 100
initial_lr = 0.01 #0.0001 # initial learning rate

model.compile(
    loss="binary_crossentropy",
    optimizer= keras.optimizers.Adam(learning_rate=initial_lr),
    metrics=["acc"],
)

# Define callbacks.
checkpoint_cb = keras.callbacks.ModelCheckpoint(
    "3d_image_classification_big.h5", save_best_only=True
)
early_stopping_cb = keras.callbacks.EarlyStopping(monitor="val_acc", patience=50)

# Train the model, doing validation at the end of each epoch
model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=epochs,
    shuffle=True,
    # steps_per_epoch=steps_per_epoch,
    # validation_steps=validation_steps,
    verbose=2,
    callbacks=[checkpoint_cb, early_stopping_cb]
)

print()
print("DONE")